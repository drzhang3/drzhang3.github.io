[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m currently a master student of Beijing Institute of Technology(BIT), majoring Intelligent Information Processing and Control advised by Prof.Yuanqing Xia. I\u0026rsquo;ll graduate with a Master\u0026rsquo;s degree from School of Automation, BIT, in July 2021. My research interests lie within deep learning for Graph Neural network and Natural Language Processing. I am also involved in Machine Learning problems like optimization algorithms. I was a research intern of Simple Log Serive(SLS) Team in Ali Cloud, advised by Guiyang Liu. I\u0026rsquo;ll be a research intern of Wechat in Tecent during 2020 summer.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://Drzhang3.github.io/author/hongwei-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/hongwei-zhang/","section":"authors","summary":"I\u0026rsquo;m currently a master student of Beijing Institute of Technology(BIT), majoring Intelligent Information Processing and Control advised by Prof.Yuanqing Xia. I\u0026rsquo;ll graduate with a Master\u0026rsquo;s degree from School of Automation, BIT, in July 2021.","tags":null,"title":"Hongwei Zhang","type":"authors"},{"authors":null,"categories":null,"content":"开了一个仓库，主要是近两三年各大顶会的针对深度时序数据的相关论文和代码，包括时序建模、预测和异常检测，欢迎大家开issue/pull requset提供新的reference。 地址 https://github.com/drzhang3/DeepTimeSeriesModel\n其中部分文章之前有仔细研究过，如果有新的idea也欢迎留言交流。\n","date":1575993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575993600,"objectID":"4b04a81f81fda3933260a9dd8c510da2","permalink":"https://Drzhang3.github.io/post/timeseries/","publishdate":"2019-12-11T00:00:00+08:00","relpermalink":"/post/timeseries/","section":"post","summary":"开了一个仓库，主要是近两三年各大顶会的针对深度时序数据的相关论文和代码，包括时序建模、预测和异常检测，欢迎大家开issue/pull requset提供新的reference。","tags":["PaperList"],"title":"Deep Time Series Model PaperList","type":"post"},{"authors":null,"categories":null,"content":"从杭州回来短短的三四天，我把自己安排的很忙，每天有不同的饭局，着实享受了一把时间按小时分配的舒服哈哈哈哈，真的觉得时间规划细致一点，一天可以干很多事情。今天一大早跟室友去打球，小菜鸡好久没动，没蹦哒一会儿就累的全身湿透，一转眼就又要走了，距离去年建站，差不多也一年，于是想整理整理一年来在科研这条路上的感受。\n一年确实快，当初立的flag如今打在脸上啪啪作响。果然是只靠着热情干不成一件事。看看上一篇文的更新时间，我就觉得从那会儿到现在，好像一点产出都没有。\n要说过去一年的收获，大约就是走的弯路太多，诸多不顺，直到最近一段时间，才得以窥探到科研大门之一二。\n这一年paper看的不多，知乎倒是逛的不少。一方面由衷感慨知乎大概是ML，DL方向国内最好的交流平台，一方面也感觉到碎片化的阅读本身的局限性。等看多了自己也想提笔整理一些知识分享给别人的时候，才觉得，以经常更blog而不是经常更知识储备为目标还是挺幼稚的。\n我很感激那些在我迷茫时候给我灌鸡汤的人，让我愿意跳出自己的舒适圈，去尝试没有涉足的东西。建站之初，是某天无聊，关注了一个差不多同龄的挺优秀的博主，想着自己也能完全复制人家的成长路径。于是看着她分享经历和故事，自己也受刺激一时心血来潮想做个学习记录以及知识分享，当时觉得有一个自己的blog真酷，后来才发现，人家是10年码农，自己是个半路二吊子，人家今年本科毕业的时候，作为优秀毕业生代表发言，自己丢在人群中再普通不过，就觉得，转型的这条路真的是很艰难，心理上的压力真的很考验一个人。虽然现在我早已度过了那个阶段，但那种感觉现在仍时不时推着我往前走。我也说不准要是当时选择另外一条路，现在可能就在中科院某所养老了hhh，那自是另外一段不可捉摸的事，暂且不表。\n后来知乎逛多了，又关注了几个颇有影响力的人，其中一位在我目前的research里颇有造诣 ，又一位自称刚入门科研的人，去年一年中了4篇A会\u0026hellip;\u0026hellip;这些人的激励让我一直保持了思考，这是件好事儿，一方面也让我陷入了对自己更大的压力当中。现在觉得，要是我当时不换专业，是不是也可以一年搞几篇出来（哈哈哈哈哈哈这是不可能的，毕竟还是小菜鸡）\n6月份的时候，我都差点换了方向，给相中的一位学术大牛，连邮件内容都准备好了，正在修改简历的时候，毫无征兆的有了大厂实习的机会，真禁不住让人感慨，人的命运除了个人奋斗，确实要考虑历史的进程\u0026hellip;\u0026hellip;\n很感谢这次实习，让我成长很快，遇到nice的工程师带着，可以随时讨论问题的氛围，紧张有序的工作节奏，以及一些经验，让我收益颇深，希望接下来几个月可以脱胎换骨hh\n希望科研的第二年真的是在踏踏实实做科研，希望少立flag，多出一些有用的idea，多做一些能真正work的工作，大佬们勿喷，求带小菜鸡做科研发paper\n","date":1567267200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567267200,"objectID":"236a40aaf8e28b258151162d1406821e","permalink":"https://Drzhang3.github.io/post/summary/","publishdate":"2019-09-01T00:00:00+08:00","relpermalink":"/post/summary/","section":"post","summary":"科研小白心路历程——从入门到入门","tags":["My experience"],"title":"科研入门总结","type":"post"},{"authors":null,"categories":null,"content":"最近在做一个有关物联网的项目。零零散散看了好久，笔者也不是物联网科班出身，因此走了不少弯路。最近想到可以将学习过程记录下来，既方便自己翻阅查看，避免浪费时间重复搜索，也许还可以为后来人提供一点经验和想法。\n项目具体内容是：\n 收集海量终端设备（传感器）的数据，即时上传到物联网云平台（需要自己构建），在上面完成数据分析和处理，并推送到移动终端上。\n 从开发角度讲，整个项目主要包括以下几部分内容：\n 无线接入（设备端） 网关 身份设备管理+数据分析+消息推送（云端） 前端开发（手机） #物联网简介 物联网，Internet of Things，顾名思义就是基于互联网基础上的物与物的“连接”。这其中涉及到两层含义，一是物联网构建在互联网基础之上，二是可连接之物的范围大大增加，甚至可以说是任何物品。在2017年10月召开的中国计算机大会上,梅宏院士作的报告中就有提到“万物皆可互联”。所谓“连接”本质上就是信息交换和通信。 如下图是物联网系统的架构示意图。   从应用的角度出发，物联网系统可分解为物联设备、网关、云端、用户终端。 物联设备可分为两类，一种因其支持TCP/IP，可以直接接入物联网，如wifi、GPRS/3G/4G等，一种则需要网关（实现协议转换），如zigbee，蓝牙等。 实际上，对于一些场景，比如终端设备通过蓝牙通信的话，则手机本身就相当于网关。笔者的项目中，设备与手机之间通过蓝牙通信，终端数据通过手机的wifi或者3G/4G模块与云端通信。因此只需专注于云端服务的开发上。\n#“协议”\n在介绍云端消息推送服务需要用到的协议之前，鉴于笔者经常混淆各类协议，后来看到OSI七层结构时，茅塞顿开。原来平时所谓不同的协议是针对不同层讨论的，将不同层之间的协议放在一起讨论对比是初学者最容易犯的错误。 在此，简单介绍下 OSI七层结构，熟悉的读者可自行跳过。 OSI是一个开放性的通信系统互连参考模型，是一个定义得非常好的协议规范。OSI模型有7层结构，每层都可以有几个子层。 OSI的7层从上到下分别是 其中高层（即7、6、5、4层）定义了应用程序的功能，下面3层（即3、2、1层）主要面向通过网络的端到端的数据流。 回到刚才的问题，消息推送服务建立在应用层协议上。物联网领域常见的有HTTP、Websocket、MQTT、CoAP等。\n下面对这几种常见的消息推送技术作简要介绍。\n##HTTP与HTTPs\n这是一种B/S通信模式。浏览器通过http协议的url向服务器发起请求，服务器将url对应的html内容回传给浏览器。至于具体如何请求，解析以及回传，包括https涉及的加密等，参见 这篇文章。 回到之前的应用上，基于HTTP协议，云端只需使用相应技术设计前端网页。手机需要模拟HTTP协议向服务器发送请求，一般情况下，请求格式为xml或者json。云端接受请求后，使用Http协议的servlet进行响应，返回xml或json格式的消息。\n从上面的描述中看出，这种通信方式的缺点也就是它的工作方式：请求——响应。云端不会主动向客户端推送消息，是一种单向通信方式。\n在物联网领域往往需要双向通信，需要云端主动向终端发送消息，这里的终端既可以指手机等人机接口，也可以指云控制系统中的执行机构或具有自动控制功能的传感器网络。 ##WebSocket\n鉴于HTTP协议单工通信的在一些应用上的不足，工程师们发明了WebSocket。维基百科介绍如下：\n WebSocket是一种在单个TCP连接上进行全双工通讯的协议。WebSocket使得客户端和服务器之间的数据交换变得更加简单，允许服务端主动向客户端推送数据。在WebSocket API中，浏览器和服务器只需要完成一次握手，两者之间就直接可以创建持久性的连接，并进行双向数据传输。\n 在这里贴一篇 博文，作者详细介绍了相关内容，有兴趣的读者可以进一步了解和学习。 ##MQTT MQTT（Message Queuing Telemetry Transport，消息队列遥测传输），是一种“发布/订阅”模式的协议。 在此列举常用的MQTT资料，供读者参考。 MQTT协议服务端： https://github.com/mqtt/mqtt.github.io/wiki/servers MQTT协议类库： https://github.com/mqtt/mqtt.github.io/wiki/libraries MQTT协议官网： http://mqtt.org/\n","date":1542902400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542902400,"objectID":"b54d4a8cc651e97ef211271d1afe35a5","permalink":"https://Drzhang3.github.io/post/iot/","publishdate":"2018-11-23T00:00:00+08:00","relpermalink":"/post/iot/","section":"post","summary":"最近在做一个有关物联网的项目。零零散散看了好久，笔者也不是物联网科班出身，因此走了不少弯路。最近想到可以将学习过程记录下来，既方便自己翻阅查看，避免浪费时间重复搜索，也许还可以为后来人提供一点经验和想法。","tags":["IOT"],"title":"物联网入门","type":"post"},{"authors":null,"categories":null,"content":"感知机算法提出于1957年，是对生物神经细胞的简单抽象。如果你稍微了解一点神经网络的话，就会知道神经网络又称多层感知机（Multi-Layer Perceptron,MLP)。神经网络之所以能够发挥强大的作用就是在感知机的结构上作了调整。而支持向量机(Supported Vector Machine,SVM)算法的基础就是感知机。因此，有必要了解基础的感知机算法（Perceptron Learning Algorithm,PLA）思想及其原理。\n##基本思想 感知机是一种线性分类模型。也就是说针对的是线性可分的数据。最常见的例子就是二维平面内的一堆数据可以通过一条直线分割开。 显然，这种分割并不是唯一 的，PLA算法就是找到其中一种可分的方案。如果推广至高维空间，就是寻找一个超平面。学习的过程就是寻找合适的损失函数，并尽可能使损失函数最小化。 给定数据集$D={(x_1,y_1),(x_2,y_2),\u0026hellip;,(x_m,y_m)}$，其中$x_i \\in R^n,y_i \\in {-1,+1},i=1,2,\u0026hellip;,N$寻找超平面$$\\omega^T x+b=0 \\tag{1.1}$$ 其中$\\omega$和$b$就是要最终确定的超平面的参数。$w$表示超平面的法向量，$b$表示超平面的截距。则空间中任意一点$x_0 \\in R^{n}$到超平面的距离可以表示为$$\\frac{|\\omega^T x_0+b|}{||\\omega||} \\tag{1.2}$$. 对于分类任务来说，我们更关心的是不能被正确分类的点，因此，对于误分类的数据点$(x_i,y_i)$来说,有 $$-y_i(\\omega x_{i} +b)\u0026gt;0 \\tag{1.3}$$ 其到超平面的距离为$$-\\frac{1}{||\\omega||}{y_i (\\omega x_{i} +b)} \\tag{1.4}$$ 则整个学习过程中的损失函数可以如下定义，即考虑所有误分类点到超平面的距离并使其最小。 $$\\frac{1}{||\\omega||}\\sum_{x_i \\in M} {-y_i (\\omega x_i +b)} \\tag{1.5}$$ 对上式做简化，不考虑$\\frac{1}{||\\omega||}$，(之后的SVM推导时会介绍为什么可以这样做)，则上述式子变为 $$L(\\omega,b)=\\sum_{x_i \\in M} {-y_i (\\omega x_i +b)} \\tag{1.6}$$ 问题转化为求解 $$\\min_{w,b} L(\\omega,b)=\\sum_{x_i \\in M} {-y_i (\\omega x_i +b)} \\tag{1.7}$$ ##求解 在求解最优化问题时，常用梯度下降、拟牛顿法等数值计算方法。具体地，在这里采取随机梯度下降法。区别于传统的梯度下降，随机梯度下降每次并不是计算所有误分类点，而是在每次迭代过程中，随机选取一个点，按下式计算梯度。 $$\\nabla_{\\omega}L(\\omega,b)=-\\sum_{x_i \\in M}y_ix_i \\tag{2.1}$$ $$\\nabla_bL(\\omega,b)=-\\sum_{x_i \\in M}y_i \\tag{2.2}$$ 随机选取误分类点$(x_i,y_i)$，更新参数$\\omega,b$： $$\\omega \\leftarrow \\omega+\\eta y_ix_i \\tag{2.3}$$ $$b \\leftarrow b+\\eta y_i \\tag{2.4}$$ 其中，$\\eta \\in (0,1]$称之为学习率。 则感知机算法描述如下： 输入：$D={(x_1,y_1),(x_2,y_2),\u0026hellip;,(x_m,y_m)},y_i \\in {-1,+1}$；学习率$\\eta \\in (0,1]$； 输出：$\\omega,b$；感知机模型$$f(x)=sign(\\omega x+b)$$\n 初始化$\\omega,b$ 随机选取数据点$(x_i,y_i)$ 若出现误分类，即$-y_i(\\omega x_{i} +b) \\geq 0$ $$\\omega \\leftarrow \\omega+\\eta y_ix_i $$ $$b \\leftarrow b+\\eta y_i $$ 转至2，直至没有误分类点。  ##收敛性 所谓收敛，指的是经过有限次迭代以后，可以得到一个正确划分正负样本的超平面。 理论推导证明，对于线性可分的数据，感知机算法迭代的次数，也就是误分类数据点的次数有一个上界。 现在为了推导方便，对感知机模型作简化。 原始形式的输出模型为$$f(x)=sign(\\omega^Tx+b) \\tag{3.1}$$ 将其表示为向量形式\n\\begin{equation} \\begin{aligned} f(x)\u0026amp;=sign(\\omega^Tx+b)\\\n\u0026amp;=sign(\\sum_{i=1}^n \\omega_ix_i+b)\\\n\u0026amp;=sign(\\sum_{i=1}^n \\omega_ix_i+\\underbrace{b}_{\\omega_0}\\cdot \\underbrace{1}_{x_0})\\\n\u0026amp;=sign(\\sum_{i=0}^n\\omega_ix_i)\\\n\u0026amp;=sign(\\omega^T\\hat{x})=sign(w\\hat{x}) \\end{aligned} \\end{equation}\\tag{3.2}\n在证明收敛上界之前，先说明几个简单的性质： 1 最终求得的分类超平面为$w_f\\hat{x}=0$将数据完全分开，则 $$y_i(w_f\\hat{x_i})\\geq \\min_{i} y_i(w_f\\hat{x_i})\u0026gt;0 \\tag{3.3}$$ 记$$\\gamma= \\min_{i} y_i(w_f\\hat{x_i}) \\tag{3.4}$$ 2 算法从$w_0=0$开始，则设$w_{k-1}$为第k个被误分类的数据，有 $$sign(w_{k-1}\\hat{x_i})\\neq y_i \\Leftrightarrow y_i(w_{k-1}\\hat{x_i})\\leq 0 \\tag{3.5}$$ 3 权值更新策略 $$w_k\\leftarrow w_{k-1}+\\eta y_i\\hat{x_i} \\tag{3.6}$$ 算法迭代的过程就是$w_k$向$w_f$接近的过程。对于两个向量，我们可以通过下式来衡量两个向量接近的程度。 $$\\frac{w_f}{||w_f||}\\frac{w_k}{||w_k||} \\tag{3.7}$$\n先给出结论 $$1\\geq\\frac{w_f}{||w_f||}\\frac{w_k}{||w_k||}\\geq \\sqrt{k}·constant \\tag{3.8}$$ 若上式成立，显然，误分类次数也即迭代次数有上界，则算法收敛性得证。 接下来一步一步证明。 $$\\begin{align*}w_fw_k\u0026amp;=w_f(w_{k-1}+\\eta y_i\\hat{x_i})\\\u0026amp;=w_fw_{k-1}+\\eta w_fy_i\\hat{x_i}\\\u0026amp;\\geq w_fw_{k-1}+\\eta \\gamma\\\u0026amp;\\geq w_fw_{k-2}+2\\eta \\gamma\\\u0026amp;\\geq \u0026hellip;\\\u0026amp;\\geq \\underbrace{w_fw_0}_0+k\\eta \\gamma\\\u0026amp;=k\\eta \\gamma\\end{align*} \\tag{3.9}$$\n$$\\begin{align*}||w_k||^2\u0026amp;=||w_{k-1}||^2+2\\eta y_iw_{k-1}\\hat{x_i}+\\eta^2||\\hat{x}_i||^2\\\u0026amp;\\leq ||w_{k-1}||^2+\\eta^2||\\hat{x}_i||^2\\\u0026amp;\\leq ||w_{k-1}||^2+\\eta^2R^2\\\u0026amp;\\leq ||w_{k-2}||^2+2\\eta^2R^2\\\u0026amp;\\leq k\\eta^2R^2\\end{align*} \\tag{3.10}$$ 在上式的推导中，$$R=\\max_{n} ||x_i||^2 \\tag{3.11}$$ 将式(3.9)，式(3.10)的结论代入式(3.7)，得 $$1\\geq\\frac{w_f}{||w_f||}\\frac{w_k}{||w_k||}\\geq \\frac{k\\eta \\gamma}{||w_f||\\sqrt{k}\\eta R}\\geq \\sqrt{k}\\frac{\\gamma}{||w_f||R} \\tag{3.12}$$\n$$\\begin{align*}k\u0026amp;\\leq \\frac{||w_f||^2R^2}{\\gamma^2}\\\u0026amp;\\leq \\frac{||w_f||^2 R^2}{(\\min_{i} y_iw_f\\hat{x_i})^2}\\\u0026amp;\\leq \\frac{R^2}{(\\min_{i} y_i\\frac{w_f}{||w_f||}\\hat{x_i})^2}\\\u0026amp;= \\frac{R^2}{\\rho^2}\\end{align*} \\tag{3.13}$$ 至此，收敛性证明结束。 ##对偶形式 对偶是优化理论里的问题。通常情况下，在某种条件下，原问题可以转化为等价的更容易求解的对偶问题。这里不具体展开讲对偶理论，有兴趣的读者可以自行查阅**拉格朗日对偶性**相关理论，在后续的SVM的介绍中，笔者也会进一步解释这个概念。 这里对感知机中的原问题与对偶问题做简单梳理： 因为感知机算法是误分类点驱动的，所以，对模型参数更新有贡献的数据一定是被误分类的点。假设样本点$(x_i,y_i)$在整个更新过程中被误分类$n_i$次，则式$(2.3),(2.4)$可以表示为 $$\\omega=\\sum_{i=1}^{N} n_i\\eta y_ix_i \\tag{4.1}$$ $$b=\\sum_{i=1}^{N} n_i\\eta y_i \\tag{4.2}$$ $n_i$越大，表明第$i$个数据点离最终要求的超平面越近，超平面只要稍微动一下，该点就从正样本变成了负样本或者相反，这样的点往往对最终的超平面影响越大。 将式$(4.1),(4.2)$代入到感知器模型$f(x)=sign(\\omega x+b)$中，得到： $$f(x)=sign(\\sum_{j=1}^{N}n_j\\eta y_jx_j·x+\\sum_{j=1}^{N}n_j\\eta y_j ) \\tag{4.3}$$ 这时，模型中的参数由原来的$\\omega$和$b$变成了$n_i$ 与原问题相对应，对偶问题描述如下： 输入：$D={(x_1,y_1),(x_2,y_2),\u0026hellip;,(x_m,y_m)},y_i \\in {-1,+1}$；学习率$\\eta \\in (0,1]$； 输出：$n_i$;感知机模型$$f(x)=sign(\\sum_{j=1}^{N}n_j\\eta y_jx_j·x+\\sum_{j=1}^{N}n_j\\eta y_j )$$\n 初始化$n_i=0,\\forall i=1,2,\u0026hellip;,N$ 随机选取数据点$(x_i,y_i)$ 若出现误分类，即$$\\sum_{j=1}^{N}n_j\\eta y_jx_j·x_i+\\sum_{j=1}^{N}n_j\\eta y_j \\leq0$$时，按照下式更新： $$n_i \\leftarrow n_i+1$$ 转至2，直至没有误分类点。 从以上推导可以看出，PLA的原始形式和对偶形式本质上一样，但从具体的计算过程来看，数据点仅以向量内积的形式出现。也就是说，可以预先计算数据点之间的内积并以矩阵的形式存储，则这个矩阵就是Gram矩阵。 $$G=[x_i·x_j]_{M×N} \\tag{4.4}$$ #结束 为可视化方便，在二维平面产生数据。运行程序，可将数据集分为两类。如下图 关于PLA算法，其缺点是不能处理线性不可分数据。 笔者在开始说，感知机算法输出的超平面并不唯一，这是因为算法执行的时候，由于参数初值的不同以及每次误分类点出现的顺序不同（随机梯度下降的结果），导致对一个数据集而言，有很多种分割方案。我们需要的往往是这些分割中最“好”的那一个。那么如何定义“好”，并让算法每次都给出“好”的超平面呢？这就是之前多次提到的SVM算法。之后的博客会介绍SVM算法。 由于作者水平有限，文中难免有错误和不当之处，欢迎大佬们批评指正！ #References 1 李航. 统计学习方法[M]. 清华大学出版社, 2012. 2 林轩田.《机器学习基石》视频资料. 3 维基百科编者.感知器[G/OL].维基百科,2018(20181014)[2018-10-14].  ","date":1524067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524067200,"objectID":"52f5414123d16feb290198c57ba5b59b","permalink":"https://Drzhang3.github.io/post/perceptron/","publishdate":"2018-04-19T00:00:00+08:00","relpermalink":"/post/perceptron/","section":"post","summary":"经典的感知机算法的推导与收敛性证明。","tags":["Algorithms"],"title":"感知机推导与证明","type":"post"}]